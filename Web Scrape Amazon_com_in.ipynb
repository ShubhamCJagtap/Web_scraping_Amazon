{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3b034472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Amazon_web_scrape.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Amazon_web_scrape.py\n",
    "\n",
    "\"\"\"\n",
    "    Developed By : Shubham Jagtap\n",
    "    linkedIn : https://www.linkedin.com/in/shubham-jagtap-scj4497/\n",
    "    \n",
    "    Please install Beautiful Soup and Selenium 4\n",
    "    \n",
    "    How to use program:\n",
    "    1. Run the File\n",
    "    2. Provide the item name you want to scrape eg. laptop, mobile, etc\n",
    "    3. Provide pages you want to scrape\n",
    "    \n",
    "    The program will generate csv for each webpage\n",
    "    \n",
    "    Enjoy Amazon scraping\n",
    "\"\"\"\n",
    "#----------------------------------------------------------\n",
    "#please install following packages if not present\n",
    "\n",
    "#!pip3 install bs4\n",
    "#!pip3 install Selenium  \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "def get_url(search_term):\n",
    "    print('Searching for {} on'.format(search_term))\n",
    "    template = \"https://www.amazon.in/s?k={}&crid=3VP0SAVW3TZS7&sprefix={}%2Caps%2C214&ref=nb_sb_noss_1\"\n",
    "    search_term = search_term.replace(' ','+')\n",
    "    #print(template.format(search_term,search_term))\n",
    "    return template.format(search_term,search_term)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def start_browser():\n",
    "    \"\"\"\n",
    "    Start the Edge web browser \n",
    "    \"\"\"\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.use_chromium = True\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    driver = webdriver.Edge(options=options)\n",
    "    print('Browser Started...')\n",
    "    return driver\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "def extract_record(item):\n",
    "    \"\"\"Extract and return data from a single record\"\"\"\n",
    "    atag = item.h2.a\n",
    "    description = atag.text.strip()\n",
    "    url = 'https://www.amazon.in' + atag.get('href')\n",
    "    price_parent = item.find('span','a-price')\n",
    "    try:\n",
    "        price = price_parent.find('span','a-offscreen').text\n",
    "    except:\n",
    "        price = \"\"\n",
    "    try:\n",
    "        rating = item.i.text\n",
    "    except:\n",
    "        rating = \"\"\n",
    "    try:\n",
    "        review_count = item.find('span',{'class':'s-underline-text'}).text\n",
    "    except:\n",
    "        review_count = \"\"\n",
    "    result = (description,price,rating,review_count,url)\n",
    "    return result\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "def get_details(records):\n",
    "    \"\"\"\n",
    "    Extract the details and return tuple of (Details,price,ratings,reviews,url)\n",
    "    \"\"\"\n",
    "    Details = []\n",
    "    price = []\n",
    "    ratings = []\n",
    "    reviews = []   \n",
    "    url = []\n",
    "    for i in records:\n",
    "        Details.append(i[0])\n",
    "        price.append(i[1])\n",
    "        ratings.append(i[2])\n",
    "        reviews.append(i[3])\n",
    "        url.append(i[4])\n",
    "    return (Details,price,ratings,reviews,url)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "def final(soup,page_no):\n",
    "    \"\"\"\n",
    "    Pass the extracted soup and page number to extract\n",
    "    All the details will be stored in csv format\n",
    "    \"\"\"\n",
    "    print('Getting data from {}'.format(page_no))\n",
    "    records = []\n",
    "    results = soup.find_all('div',{'data-component-type':'s-search-result'})\n",
    "    for item in results:\n",
    "        record = extract_record(item)\n",
    "        if record:\n",
    "            records.append(record) \n",
    "    filename='Page-'+str(page_no)+str('.csv')\n",
    "    with open(filename,'w',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Description','Price','Rating','ReviewCount','Url'])\n",
    "        writer.writerows(records)\n",
    "    print('Done page {}'.format(page_no))\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "## Main Code ##\n",
    "\n",
    "item = str(input(\"Please Enter searching Item :  \"))\n",
    "pages = int(input(\"Enter Number of pages you want to extract : \"))\n",
    "driver = start_browser()\n",
    "for i in range(pages):\n",
    "    url = get_url(item)\n",
    "    if i>0:\n",
    "        url=url+str('&page=')+str(i+1)\n",
    "    else:\n",
    "        pass\n",
    "    driver.get(url)\n",
    "    print(url)\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    final(soup,i)\n",
    "    if pages>15:\n",
    "        print('Current Page limit reached 15')\n",
    "        driver.quit()\n",
    "        print('Browser Closed')\n",
    "        break\n",
    "driver.quit()\n",
    "print('Browser Closed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4a9e10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(search_term):\n",
    "    print('Searching for {} on'.format(search_term))\n",
    "    template = \"https://www.amazon.in/s?k={}&crid=3VP0SAVW3TZS7&sprefix={}%2Caps%2C214&ref=nb_sb_noss_1\"\n",
    "    search_term = search_term.replace(' ','+')\n",
    "    #print(template.format(search_term,search_term))\n",
    "    return template.format(search_term,search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3ad8079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_browser():\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.use_chromium = True\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    driver = webdriver.Edge(options=options)\n",
    "    print('Browser Started...')\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "414f7dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_record(item):\n",
    "    \"\"\"Extract and return data from a single record\"\"\"\n",
    "    atag = item.h2.a\n",
    "    description = atag.text.strip()\n",
    "    url = 'https://www.amazon.in' + atag.get('href')\n",
    "    price_parent = item.find('span','a-price')\n",
    "    try:\n",
    "        price = price_parent.find('span','a-offscreen').text\n",
    "    except:\n",
    "        price = \"\"\n",
    "    try:\n",
    "        rating = item.i.text\n",
    "    except:\n",
    "        rating = \"\"\n",
    "    try:\n",
    "        review_count = item.find('span',{'class':'s-underline-text'}).text\n",
    "    except:\n",
    "        review_count = \"\"\n",
    "    result = (description,price,rating,review_count,url)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "33fa92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details(records):\n",
    "    Details = []\n",
    "    price = []\n",
    "    ratings = []\n",
    "    reviews = []   \n",
    "    url = []\n",
    "    for i in records:\n",
    "        Details.append(i[0])\n",
    "        price.append(i[1])\n",
    "        ratings.append(i[2])\n",
    "        reviews.append(i[3])\n",
    "        url.append(i[4])\n",
    "    return (Details,price,ratings,reviews,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "aa1bd1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final(soup,page_no):\n",
    "    print('Getting data from {}'.format(page_no))\n",
    "    records = []\n",
    "    results = soup.find_all('div',{'data-component-type':'s-search-result'})\n",
    "    for item in results:\n",
    "        record = extract_record(item)\n",
    "        if record:\n",
    "            records.append(record) \n",
    "    filename='Page-'+str(page_no)+str('.csv')\n",
    "    with open(filename,'w',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Description','Price','Rating','ReviewCount','Url'])\n",
    "        writer.writerows(records)\n",
    "    print('Done page {}'.format(page_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "09ac5b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter searching Item :  laptop\n",
      "Enter Number of pages you want to extract : 4\n",
      "Browser Started...\n",
      "Searching for laptop on\n",
      "https://www.amazon.in/s?k=laptop&crid=3VP0SAVW3TZS7&sprefix=laptop%2Caps%2C214&ref=nb_sb_noss_1\n",
      "Getting data from 0\n",
      "Done page 0\n",
      "Searching for laptop on\n",
      "https://www.amazon.in/s?k=laptop&crid=3VP0SAVW3TZS7&sprefix=laptop%2Caps%2C214&ref=nb_sb_noss_1&page=2\n",
      "Getting data from 1\n",
      "Done page 1\n",
      "Searching for laptop on\n",
      "https://www.amazon.in/s?k=laptop&crid=3VP0SAVW3TZS7&sprefix=laptop%2Caps%2C214&ref=nb_sb_noss_1&page=3\n",
      "Getting data from 2\n",
      "Done page 2\n",
      "Searching for laptop on\n",
      "https://www.amazon.in/s?k=laptop&crid=3VP0SAVW3TZS7&sprefix=laptop%2Caps%2C214&ref=nb_sb_noss_1&page=4\n",
      "Getting data from 3\n",
      "Done page 3\n",
      "Browser Closed\n"
     ]
    }
   ],
   "source": [
    "item = str(input(\"Please Enter searching Item :  \"))\n",
    "pages = int(input(\"Enter Number of pages you want to extract : \"))\n",
    "driver = start_browser()\n",
    "for i in range(pages):\n",
    "    url = get_url(item)\n",
    "    if i>0:\n",
    "        url=url+str('&page=')+str(i+1)\n",
    "    else:\n",
    "        pass\n",
    "    driver.get(url)\n",
    "    print(url)\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    final(soup,i)\n",
    "    if pages>15:\n",
    "        print('Current Page limit reached 15')\n",
    "        driver.quit()\n",
    "        print('Browser Closed')\n",
    "        break\n",
    "driver.quit()\n",
    "print('Browser Closed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009b941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
